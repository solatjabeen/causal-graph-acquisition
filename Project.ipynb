{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import textacy.preprocessing\n",
    "import textacy.resources\n",
    "#import textacy.keyterms\n",
    "import textacy.ke\n",
    "import neuralcoref\n",
    "from spacy.symbols import ORTH, POS, NOUN, VERB,PRON\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from networkx.readwrite import json_graph;\n",
    "import json\n",
    "from afinn import Afinn\n",
    "afn = Afinn()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from allennlp_models.pretrained import load_predictor\n",
    "predictor = load_predictor(\"roberta-sst\")\n",
    "from sentistrength import PySentiStr\n",
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath('C:/SentiStrengthCom.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('C:/SentStrength_Data_Sept2011/')\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import collections\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read text file and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from spacy.lang.en import English\n",
    "\n",
    "file_name = './Text/Simplified Narrative.txt'\n",
    "#file_name = 'input.txt'\n",
    "narrative = open(file_name, encoding=\"utf8\").read()\n",
    "\n",
    "narrative = textacy.preprocessing.normalize_quotation_marks(narrative)\n",
    "#narrative = textacy.preprocessing.remove_punctuation(narrative, marks=\",;:\")\n",
    "#narrative = textacy.preprocessing.normalize_whitespace(narrative)\n",
    "#narrative = textacy.make_spacy_doc(narrative)\n",
    "narrative = narrative.lower()\n",
    "narrative = nlp(narrative)\n",
    "\n",
    "#narrative._.coref_clusters\n",
    "#narrative._.coref_resolved\n",
    "\n",
    "print('Original Narrative:')\n",
    "print(narrative)\n",
    "#print('\\n')\n",
    "print('Extracted Coreferences:')\n",
    "print(narrative._.coref_clusters)\n",
    "print('\\n')\n",
    "narrative = narrative._.coref_resolved\n",
    "print('Narrarive after resolving coreferences:')\n",
    "print(narrative)\n",
    "\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "narrative = nlp(narrative)\n",
    "#sentences = [sent.string.strip() for sent in narrative.sents]\n",
    "#narrative = nlp(narrative)\n",
    "\n",
    "print('\\nSentences:\\n')\n",
    "for sent in narrative.sents:\n",
    "    print(sent.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Subject-Verb-Object triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sent in narrative.sents:\n",
    "    #print(\"Heyyyy!\")\n",
    "    print(sent)\n",
    "    for tok in sent:\n",
    "        print(tok.text,tok.pos_,tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new\n",
    "finalList = []\n",
    "sentences = []\n",
    "ncl = []\n",
    "nncl = [()]\n",
    "checkPass = False\n",
    "rootCheck = False\n",
    "varForm1 = False\n",
    "varForm2 = False\n",
    "#tuple =('a','b','c')\n",
    "\n",
    "#print(tuple[0])\n",
    "\n",
    "for sent in narrative.sents:\n",
    "    print(\"Sentence:\")\n",
    "    print(sent)\n",
    "    \n",
    "    print(\"Noun Chunks:\")\n",
    "    for nc in sent.noun_chunks:\n",
    "        print (nc)\n",
    "        ncl.append(nc)\n",
    "    print (\"Number of noun chunks: \", len(ncl))\n",
    "    \n",
    "    triplets = textacy.extract.subject_verb_object_triples(sent)\n",
    "    triplets = list(triplets)\n",
    "    if len(triplets) > 0:\n",
    "        print(\"Triples from textacy:\")\n",
    "        for t in triplets:\n",
    "            subject = str(t[0])\n",
    "            objec = str(t[2])\n",
    "            for chunk in ncl:\n",
    "                for cToken in chunk:\n",
    "                    if str(cToken) == str(t[0]):\n",
    "                        subject = str(chunk)\n",
    "                    if str(cToken) == str(t[2]):\n",
    "                        objec = str(chunk)\n",
    "            tup = (subject,str(t[1]),objec)\n",
    "            print(tup)\n",
    "            finalList.append(tup)\n",
    "            sentences.append(sent)\n",
    "        if len(ncl) == 3:\n",
    "            nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "            nncl.append((ncl[0],triplets[0][1],ncl[2]))\n",
    "            print(\"Triples other than textacy:\")\n",
    "            print(nncl[0])\n",
    "            print(nncl[1])\n",
    "            finalList.append(nncl[0])\n",
    "            sentences.append(sent)\n",
    "            finalList.append(nncl[1])\n",
    "            sentences.append(sent)\n",
    "#        elif len(ncl) == 2:\n",
    "#            nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "#            print(\"Triples other than textacy:\")\n",
    "#            print(nncl[0])\n",
    "#            finalList.append(nncl[0])\n",
    "    else:\n",
    "        print(\"Method from textacy; subject_verb_object_triples extracted nothing!\")\n",
    "        for token in sent:\n",
    "            #print(token.text, token.dep_,)\n",
    "            if token.dep_ == 'nsubj':\n",
    "                sub = token.text\n",
    "                #tuple[0] = str(token.text)\n",
    "            elif token.dep_ == 'nsubjpass':\n",
    "                checkPass = True\n",
    "                sub = token.text\n",
    "            elif token.dep_ == 'ROOT':\n",
    "                #print(\"Heyyyyyyy\")\n",
    "                verb = token.text\n",
    "                rootCheck = True\n",
    "            elif token.pos_ == 'NOUN' and token.dep_ == 'conj':\n",
    "                if rootCheck is True:\n",
    "                    varForm2 = True\n",
    "                else:\n",
    "                    varForm1 = True\n",
    "                #tuple[1] = token.text\n",
    "            #elif token.dep_ == 'dobj':\n",
    "            #    obj = token.text\n",
    "                #tuple[2] = token.text\n",
    "            #    break\n",
    "            else:\n",
    "                if token.dep_ == 'dobj':\n",
    "                    obj = token.text\n",
    "                    continue\n",
    "                elif token.dep_ == 'pobj':\n",
    "                    obj = token.text\n",
    "                    continue\n",
    "                \n",
    "        for nChunk in ncl:\n",
    "            for nToken in nChunk:\n",
    "                if str(nToken) == str(sub):\n",
    "                    sub = nChunk\n",
    "                if str(nToken) == str(obj):\n",
    "                    obj = nChunk\n",
    "        if checkPass is True:\n",
    "            tuple = (obj,verb,sub)\n",
    "        else:\n",
    "            tuple = (sub,verb,obj)\n",
    "        print(\"Triple by combining nsubj, root and dobj:\")\n",
    "        print(tuple)\n",
    "        finalList.append(tuple)\n",
    "        sentences.append(sent)\n",
    "        if len(ncl) == 3:\n",
    "            if varForm1 == True:\n",
    "                nncl = [(ncl[0],verb,ncl[2])]\n",
    "                nncl.append((ncl[1],verb,ncl[2]))\n",
    "                print(\"Triples other than sub, verb and obj:\")\n",
    "                print(nncl[0])\n",
    "                print(nncl[1])\n",
    "                finalList.append(nncl[0])\n",
    "                sentences.append(sent)\n",
    "                finalList.append(nncl[1])\n",
    "                sentences.append(sent)\n",
    "            else:\n",
    "                nncl = [(ncl[0],verb,ncl[1])]\n",
    "                nncl.append((ncl[0],verb,ncl[2]))\n",
    "                print(\"Triples other than sub, verb and obj:\")\n",
    "                print(nncl[0])\n",
    "                print(nncl[1])\n",
    "                finalList.append(nncl[0])\n",
    "                sentences.append(sent)\n",
    "                finalList.append(nncl[1])\n",
    "                sentences.append(sent)\n",
    "            \n",
    "#        elif len(ncl) == 2:\n",
    "#            nncl = [(ncl[0],verb,ncl[1])]\n",
    "#            print(\"Triples other than sub, verb and obj:\")\n",
    "#            print(nncl[0])\n",
    "#            finalList.append(nncl[0])\n",
    "\n",
    "    ncl.clear()\n",
    "    nncl.clear()\n",
    "    rootCheck = False\n",
    "    checkPass = False\n",
    "    varForm1 = False\n",
    "    varForm2 = False\n",
    "    \n",
    "trips = finalList\n",
    "\n",
    "#        for token in nc:\n",
    "#            print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#old\n",
    "def SVO(narrative):\n",
    "    ncl = []\n",
    "    fl = []\n",
    "    nncl = [()]\n",
    "    for sent in narrative.sents:\n",
    "        #sent = str(sent)\n",
    "        print(\"\\n\" + str(sent))\n",
    "        for nc in sent.noun_chunks:\n",
    "            print (nc)\n",
    "            ncl.append(nc)\n",
    "        print(len(ncl))       \n",
    "        triplets = textacy.extract.subject_verb_object_triples(sent)\n",
    "        triplets = list(triplets)\n",
    "        if len(ncl) <= 3:\n",
    "            if len(triplets) == 0:\n",
    "                print(\"Method from textacy; subject_verb_object_triples extracted nothing!\")\n",
    "                for token in sent:\n",
    "                #print(token.text, token.dep_,)\n",
    "                    if token.dep_ == 'ROOT':\n",
    "                        text = token.text\n",
    "                        break\n",
    "                    #print(token.text)\n",
    "                if len(ncl) == 3:\n",
    "                        nncl = [(ncl[0],token.text,ncl[1])]\n",
    "                        nncl.append((ncl[0],token.text,ncl[2]))\n",
    "                        fl.append(nncl[0])\n",
    "                        fl.append(nncl[1])\n",
    "                elif len(ncl) == 2:\n",
    "                        nncl = [(ncl[0],token.text,ncl[1])]\n",
    "                        fl.append(nncl[0])\n",
    "                else:\n",
    "                    continue\n",
    "                print(\"Triples without textacy.\")\n",
    "                print(nncl)\n",
    "        \n",
    "            else:\n",
    "                print(triplets)\n",
    "                for t in triplets:\n",
    "                    print(t)\n",
    "                    fl.append(t)\n",
    "                if len(ncl) == 3:\n",
    "                    nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "                    nncl.append((ncl[0],triplets[0][1],ncl[2]))\n",
    "                    fl.append(nncl[0])\n",
    "                    fl.append(nncl[1])\n",
    "                    #print(nncl[0])\n",
    "                    #print(nncl[1])\n",
    "                elif len(ncl) == 2:\n",
    "                    nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "                    fl.append(nncl[0])\n",
    "                else:\n",
    "                    continue;\n",
    "                print(nncl)\n",
    "        ncl.clear()\n",
    "        nncl.clear()\n",
    "    return fl\n",
    "\n",
    "trips = SVO(narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the extracted triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, triple  in zip(sentences, trips):\n",
    "    print(sentence, triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap = nx.DiGraph()\n",
    "\n",
    "for triple in trips:\n",
    "    s = str(triple[0])    \n",
    "    d = str(triple[2])\n",
    "    cmap.add_edge(s, d, predicade=triple[1])\n",
    "\n",
    "print(len(cmap.nodes))\n",
    "print(len(cmap.edges))\n",
    "\n",
    "pos = nx.spring_layout(cmap, k=4, iterations=20)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "nx.draw(cmap, pos=pos, with_labels=True,  node_shape=\"s\",  node_color=\"none\", font_size=15,  font_color='royalblue', font_weight='bold',bbox=dict(facecolor=\"white\", alpha=0.4,boxstyle='round,pad=0.8'),labels={node: node for node in cmap.nodes()},arrows=True, arrowsize=15,width=1)\n",
    "edge_labels = nx.get_edge_attributes(cmap,'predicade')\n",
    "nx.draw_networkx_edge_labels(cmap, pos=pos, edge_labels = edge_labels, font_color='black',font_size=13)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the Knowledge Graph on \"Knowledge Graph.json\" file for                           semi-automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgeGraph = nx.DiGraph()\n",
    "\n",
    "for svo in trips:\n",
    "    s = str(svo[0])    \n",
    "    d = str(svo[2])\n",
    "    knowledgeGraph.add_node(s,id = str(s),title=str(s),x=615,y=200)\n",
    "    knowledgeGraph.add_node(d,id = str(d),title=str(d),x=615,y=200)\n",
    "    knowledgeGraph.add_edge(s, d, predicade=str(svo[1]))\n",
    "\n",
    "#print(len(knowledgeGraph.nodes))\n",
    "#print(len(knowledgeGraph.edges))\n",
    "\n",
    "data = json_graph.node_link_data(knowledgeGraph)\n",
    "\n",
    "\n",
    "out_file = open(\"Knowledge Graph.json\", \"w\")\n",
    "  \n",
    "json.dump(data, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Causal Trigger Words Dictionary along with their synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "causalWords1 = [\"forced\",\"caused\", \"resulted\", \"reason\", \"as a result of\", \"as a consequence of\", \n",
    "               \"consequence\", \"consequently\", \"affect\", \"because\", \"increase\", \"decrease\",\"due to\",\"because of\"\n",
    "               ,\"made\",\"mimimize\",\"maximize\",\"hindered\", \"displaced\", \"conspired\",\"led to\",\"activate\",\"impel\",\"inspire\",\n",
    "                \"excite\",\"quicken\",\"rouse\",\"stimulate\",\"influence\",\"determine\",\"likely\",\"probable\",\"disconnected\",\"separated\"\n",
    "                ,\"excluded\",\"after\",\"as\",\"since\",\"trigger\"]\n",
    "\n",
    "#causalWords1 = [\"forced\"]\n",
    "synonyms1 = []\n",
    "lemma_function = WordNetLemmatizer()\n",
    "\n",
    "for cw1 in causalWords1:\n",
    "    #print(cw1)\n",
    "    synonyms1.append(cw1.lower())\n",
    "    tokens1 = word_tokenize(cw1)\n",
    "    for token1, tag1 in pos_tag(tokens1):\n",
    "        lemma1 = lemma_function.lemmatize(token1)\n",
    "        #print(token1,lemma1)\n",
    "        \n",
    "        for syn in wordnet.synsets(str(lemma1)):\n",
    "            \n",
    "            for l1 in syn.lemmas():\n",
    "                #print(l1)\n",
    "                #synonyms1.append(lemma1.lower())\n",
    "                synonyms1.append(l1.name().lower())\n",
    "                #print(l1)\n",
    "                for syn1 in wordnet.synsets(str(l1.name())):\n",
    "                    for l2 in syn1.lemmas():\n",
    "                        #print(l2)\n",
    "                        synonyms1.append(l2.name().lower())\n",
    "            \n",
    "print(len(synonyms1))\n",
    "#print(set(synonyms1))\n",
    "#for synonym in synonyms1:\n",
    "#    print(synonym+\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "\n",
    "causalWords = [\"forced\",\"caused\", \"result\", \"reason\", \"as a result of\", \"as a consequence of\", \n",
    "               \"consequence\", \"consequently\", \"affect\", \"because\", \"increase\", \"decrease\",\"due to\",\"because of\"\n",
    "               ,\"made\",\"mimimize\",\"maximize\",\"led to\",\"produced\"]\n",
    "\n",
    "#causalWords = [\"as a result of\"]\n",
    "synonyms = []\n",
    "\n",
    "for cw in causalWords:\n",
    "    \n",
    "    for syn in wordnet.synsets(str(cw)):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(cw)\n",
    "            synonyms.append(l.name())\n",
    "        \n",
    "print(set(synonyms))\n",
    "print(len(synonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Causal triples from SVO triples based upon causal trigger                           words dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "causeffect = []\n",
    "causalSentences = []\n",
    "#lem = []\n",
    "#lemma_function = WordNetLemmatizer()\n",
    "for s,st in zip(sentences,trips):\n",
    "    #st = (str(st[0]),str(st[1]),str(st[2]))\n",
    "    #synonym = synonym[0]\n",
    "    tokens = word_tokenize(str(st[1]))\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemma_function.lemmatize(token)\n",
    "        #print(token,lemma)\n",
    "        #st[1] = lemma\n",
    "        #lem.append(lemma)\n",
    "        \n",
    "    for synonym in synonyms1:\n",
    "        if synonym == lemma:\n",
    "            #causeffect = [(st[0],st[1],st[2])]\n",
    "            #strTriples.append((ncl[0],token.text,st[2]))\n",
    "           # print(st)\n",
    "            causeffect.append(st)\n",
    "            causalSentences.append(s)\n",
    "            #if (len(causeffect) >0 ) and (st in causeffect == False):\n",
    "                \n",
    "            #    causeffect.append(st)\n",
    "            break\n",
    "    \n",
    "#causeffect.sort(key = lambda x: x[0])\n",
    "\n",
    "#for sortedTriple in causeffect:\n",
    "#    print(sortedTriple)\n",
    "\n",
    "for causalSentence, causalTriple  in zip(causalSentences, causeffect):\n",
    "    print(causalSentence, causalTriple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Causal Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = nx.DiGraph()\n",
    "\n",
    "for ce in causeffect:\n",
    "    s = str(ce[0])    \n",
    "    d = str(ce[2])\n",
    "    cg.add_node(s,id = str(s),title=str(s),x=615,y=200)\n",
    "    cg.add_node(d,id = str(d),title=str(d),x=615,y=200)\n",
    "    cg.add_edge(s, d, predicade=str(ce[1]))\n",
    "\n",
    "print(len(cg.nodes))\n",
    "print(len(cg.edges))\n",
    "\n",
    "pos = nx.spring_layout(cg, k=4, iterations=20)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "nx.draw(cg, pos=pos, with_labels=True,  node_shape=\"s\",  node_color=\"none\", font_size=15,  font_color='royalblue', font_weight='bold',bbox=dict(facecolor=\"white\", alpha=0.4,boxstyle='round,pad=0.8'),labels={node: node for node in cg.nodes()},arrows=True, arrowsize=15,width=1)\n",
    "edge_labels = nx.get_edge_attributes(cg,'predicade')\n",
    "nx.draw_networkx_edge_labels(cmap, pos=pos, edge_labels = edge_labels, font_color='black',font_size=13)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates from causalSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causalSentences = list(dict.fromkeys(causalSentences))\n",
    "\n",
    "causalSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Sentences' Polarity (Sentence Level) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AllenNlp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://demo.allennlp.org/sentiment-analysis/roberta-sentiment-analysis\n",
    "#https://paperswithcode.com/model/roberta-large-sst\n",
    "\n",
    "#output (Pos,neg)\n",
    "\n",
    "#sentence = \"Enemies surround Pakistan.\"\n",
    "#preds = predictor.predict(sentence)\n",
    "#print(f\"p(positive)={preds['probs'][0]:.2%}\")\n",
    "allen = []\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    preds = predictor.predict(str(sent))\n",
    "    #print(preds['label'][0])\n",
    "    label = preds['label'][0]\n",
    "    if label is '0':\n",
    "        allen.append(-1)\n",
    "    else:\n",
    "        allen.append(1)\n",
    "    \n",
    "for causalSentence, polarity  in zip(causalSentences, allen):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Afinn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/python-sentiment-analysis-using-affin/\n",
    "\n",
    "#output (Pos,neg,neural)\n",
    "\n",
    "afinn = []\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    score = afn.score(str(sent))\n",
    "    #print(score)\n",
    "    if score > 0:\n",
    "        #print('positive')\n",
    "        afinn.append(1)\n",
    "    elif score < 0:\n",
    "        #print('negative')\n",
    "        afinn.append(-1)\n",
    "    else:\n",
    "        #print('neutral')\n",
    "        afinn.append(0)\n",
    "        \n",
    "for causalSentence, polarity  in zip(causalSentences, afinn):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/cjhutto/vaderSentiment#about-the-scoring\n",
    "#output (Pos,neg,neural)\n",
    "\n",
    "vader = []\n",
    "\n",
    "def vadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    #print(vs)\n",
    "    return vs['compound']\n",
    "\n",
    "def vader_analysis(compound):\n",
    "    if compound >= 0.05:\n",
    "        vader.append(1)\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.05 :\n",
    "        vader.append(-1)\n",
    "        return 'Negative'\n",
    "        \n",
    "    elif  compound > -0.05 and compound < 0.05:\n",
    "        vader.append(0)\n",
    "        return 'Neutral'\n",
    "    \n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    compound = vadersentimentanalysis(str(sent))\n",
    "    vader_analysis(compound)\n",
    "    \n",
    "for causalSentence, polarity  in zip(causalSentences, vader):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiStrength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#http://sentistrength.wlv.ac.uk/\n",
    "#http://paper.ijcsns.org/07_book/202001/20200107.pdf\n",
    "#https://pypi.org/project/sentistrength/\n",
    "#http://sentistrength.wlv.ac.uk/results.php?text=pakistan+must+support+army+and+aq+khan.&submit=Detect+Sentiment&result=trinary\n",
    "#https://professorkhan.com/2019/03/29/sentiment-analysis-with-sentistrength/\n",
    "#output (Pos,neg)\n",
    "\n",
    "sentiStrength = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    result = senti.getSentiment(str(sent), score='binary')\n",
    "    #print(result)\n",
    "    \n",
    "    if result[0]==1:\n",
    "        #print('Positive')\n",
    "        sentiStrength.append(1)\n",
    "    elif result[0]==-1:\n",
    "        sentiStrength.append(-1)\n",
    "        #print('Negative')\n",
    "    #else:\n",
    "    #    print('Neutral')\n",
    "\n",
    "for causalSentence, polarity  in zip(causalSentences, sentiStrength):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiWordnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def sentiment_sentiwordnet(text):\n",
    "    #text = text.decode(\"utf-8\")\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    #print(raw_sentences)\n",
    "    sentiment = 0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        print(raw_sentence)\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        print(tagged_sentence)\n",
    "        \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "                \n",
    "            #lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            #if not lemma:\n",
    "            #    continue\n",
    "                \n",
    "            synsets = wn.synsets(word, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "                \n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "        # print(swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score())\n",
    "        if not tokens_count:\n",
    "            return 0\n",
    "        if sentiment>0:\n",
    "            #return \"Positive\"\n",
    "            print(\"Positive\")\n",
    "        elif sentiment==0:\n",
    "            #return \"Neutral\"\n",
    "            print(\"Neutral\")\n",
    "        else:\n",
    "            #return \"Negative\"\n",
    "            print(\"Negative\")\n",
    "    \n",
    "causalNarrative = \"\"\n",
    "for cS in causalSentences:\n",
    "    causalNarrative = causalNarrative+str(cS)+\" \"\n",
    "\n",
    "causalNarrative = nlp(causalNarrative)\n",
    "#raw_sentences = sent_tokenize(causalNarrative)\n",
    "#print(str(causalNarrative)+\"\\n\")\n",
    "#print(raw_sentences) \n",
    "#print(str(narrative))\n",
    "\n",
    "sentiment_sentiwordnet(str(narrative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiWordNet OnlyVerbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "    \n",
    "\n",
    "polarizedCauseffect = []\n",
    "for ce in causeffect:\n",
    "    print(ce[1])\n",
    "    token = nltk.word_tokenize(str(ce[1]))\n",
    "    after_tagging = nltk.pos_tag(token)\n",
    "    print(after_tagging)\n",
    "    print(len(after_tagging))\n",
    "    for tag in after_tagging:\n",
    "        if tag[1].startswith('V') or len(after_tagging)==1:\n",
    "            words = swn.senti_synsets(tag[0]) \n",
    "            answer = list(words)[0]\n",
    "            if (answer.pos_score() > answer.neg_score()):\n",
    "                mce = str(ce[1])+\"(+ve)\"\n",
    "                ce1=(ce[0],mce,ce[2])\n",
    "                polarizedCauseffect.append(ce1)\n",
    "                print(ce1)\n",
    "            elif (answer.pos_score() > 0):\n",
    "                mce = str(ce[1])+\"(-ve)\"\n",
    "                ce1=(ce[0],mce,ce[2])\n",
    "                polarizedCauseffect.append(ce1)\n",
    "                print(ce1)\n",
    "            else: \n",
    "                mce = str(ce[1])+\"(neutral)\"\n",
    "                ce1=(ce[0],mce,ce[2])\n",
    "                polarizedCauseffect.append(ce1)\n",
    "                print(ce1)\n",
    "                \n",
    "polarizedCauseffect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame (Sentence Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.DataFrame([[38.0, 2.0, 18.0, 22.0, 21, 0,0],[19, 439, 6, 452, 226,232,0]],columns=['Causal Sentence','AllenNlp','Afinn','Vader','SentiStrength','Weight','Polarity'])\n",
    "\n",
    "df2 = pd.DataFrame(list(zip(causalSentences, allen, afinn, vader,sentiStrength)), columns =['Causal Sentence', 'AllenNLP','Afinn','Vader','SentiStrength'])\n",
    "df2[\"Weight\"] = df2.sum(axis=1)\n",
    "\n",
    "\n",
    "sentPolarity = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    weight = row['Weight']\n",
    "    #print(weight)\n",
    "    if weight is -4:\n",
    "        sentPolarity.append('Strong Negative') \n",
    "        #print('Strong Negative')\n",
    "    elif weight is -3:\n",
    "        sentPolarity.append('Moderate Negative')\n",
    "        #print('Moderate Negative')\n",
    "    elif weight is -2:\n",
    "        sentPolarity.append('Mild Negative')\n",
    "        #print('Mild Negative')\n",
    "    elif weight is -1:\n",
    "        sentPolarity.append('Weak Negative')\n",
    "        #print('Weak Negative')\n",
    "    elif weight is 0:\n",
    "        sentPolarity.append('Neutral')\n",
    "        #print('Neutral')\n",
    "    elif weight is 4:\n",
    "        sentPolarity.append('Strong Positive')\n",
    "        #print('Strong Positive')\n",
    "    elif weight is 3:\n",
    "        sentPolarity.append('Moderate Positive')\n",
    "        #print('Moderate Positive')\n",
    "    elif weight is 2:\n",
    "        sentPolarity.append('Mild Positive')\n",
    "        #print('Mild Positive')\n",
    "    elif weight is 1:\n",
    "        sentPolarity.append('Weak Positive')\n",
    "        #print('Weak Positive')\n",
    "\n",
    "df2[\"Polarity\"] = sentPolarity\n",
    "df2.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Sentences' Polarity (Word Level) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AllenNlp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allen_word = []\n",
    "allen_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) != \".\":\n",
    "            #print(token)\n",
    "            preds = predictor.predict(str(token))\n",
    "            #print(preds['label'][0])\n",
    "            label = preds['label'][0]\n",
    "            if label is '0':\n",
    "                allen_tuple.append(-1)\n",
    "                #allen_word.append(-1)\n",
    "            else:\n",
    "                #allen_word.append(1)\n",
    "                allen_tuple.append(1)\n",
    "    allen_word.append(allen_tuple)\n",
    "    allen_tuple = []\n",
    "                \n",
    "allen_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Afinn] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn_word = []\n",
    "afinn_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) != \".\":\n",
    "            #print(token)\n",
    "            score = afn.score(str(token))\n",
    "            if score > 0:\n",
    "                #print('positive')\n",
    "                afinn_tuple.append(1)\n",
    "            elif score < 0:\n",
    "                #print('negative')\n",
    "                afinn_tuple.append(-1)\n",
    "            else:\n",
    "                #print('neutral')\n",
    "                afinn_tuple.append(0)\n",
    "    afinn_word.append(afinn_tuple)\n",
    "    afinn_tuple = []\n",
    "\n",
    "afinn_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vader] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vader_word = []\n",
    "vader_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) != \".\":\n",
    "            vs = analyzer.polarity_scores(str(token))\n",
    "            compound = vs['compound']\n",
    "            if compound >= 0.05:\n",
    "                vader_tuple.append(1)   \n",
    "            elif compound <= -0.05 :\n",
    "                vader_tuple.append(-1)\n",
    "            elif  compound > -0.05 and compound < 0.05:\n",
    "                vader_tuple.append(0)\n",
    "    vader_word.append(vader_tuple)\n",
    "    vader_tuple = []\n",
    "\n",
    "vader_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiStrength] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_word = []\n",
    "senti_tuple = []\n",
    "\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) != \".\":\n",
    "            #print(token)\n",
    "            result = senti.getSentiment(str(token), score='binary')\n",
    "            #print(result)\n",
    "            if result[0]==1:\n",
    "                #print('Positive')\n",
    "                senti_tuple.append(1)\n",
    "            elif result[0]==-1:\n",
    "                #print('Negative')\n",
    "                senti_tuple.append(-1)\n",
    "            else:\n",
    "                #print('Neutral')\n",
    "                senti_tuple.append(0)\n",
    "    senti_word.append(senti_tuple)\n",
    "    senti_tuple = []\n",
    "    \n",
    "senti_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame (Word Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = []\n",
    "sum_tup = []\n",
    "for i in range(0,5):\n",
    "    #dataframe_WordLevel(i)\n",
    "    for j in range(0,len(causalSentences[i])-1):\n",
    "        res = allen_word[i][j]+afinn_word[i][j]+vader_word[i][j]+senti_word[i][j]\n",
    "        #print(res)\n",
    "        sum_tup.append(res)\n",
    "    sum_list.append(sum_tup)\n",
    "    sum_tup = []\n",
    "    \n",
    "    \n",
    "tokenPolarity = []\n",
    "tokenPolarity_list = []\n",
    "\n",
    "for s in sum_list:\n",
    "    for weight in s:\n",
    "        if weight is -4:\n",
    "            tokenPolarity.append('Strong Negative') \n",
    "        elif weight is -3:\n",
    "            tokenPolarity.append('Moderate Negative')\n",
    "        elif weight is -2:\n",
    "            tokenPolarity.append('Mild Negative')\n",
    "        elif weight is -1:\n",
    "            tokenPolarity.append('Weak Negative')\n",
    "        elif weight is 0:\n",
    "            tokenPolarity.append('Neutral')\n",
    "        elif weight is 4:\n",
    "            tokenPolarity.append('Strong Positive')\n",
    "        elif weight is 3:\n",
    "            tokenPolarity.append('Moderate Positive')\n",
    "        elif weight is 2:\n",
    "            tokenPolarity.append('Mild Positive')\n",
    "        elif weight is 1:\n",
    "            tokenPolarity.append('Weak Positive')\n",
    "    tokenPolarity_list.append(tokenPolarity)\n",
    "    tokenPolarity = []\n",
    "\n",
    "maxPolarity = []\n",
    "maxPolarity_list = []\n",
    "\n",
    "for t in tokenPolarity_list:\n",
    "    counter=collections.Counter(t)\n",
    "    #print(counter)\n",
    "    #print(counter.most_common(1))\n",
    "    maxPolarity.append(counter.most_common(1)[0][0])\n",
    "    maxPolarity_list.append(maxPolarity)\n",
    "    maxPolarity = []\n",
    "\n",
    "#print(maxPolarity_list)\n",
    "    \n",
    "def dataframe_WordLevel(i):\n",
    "    df = pd.DataFrame([causalSentences[i],allen_word[i],afinn_word[i],vader_word[i],senti_word[i],sum_list[i],tokenPolarity_list[i],maxPolarity_list[i]], index =   ['Tokens','AllenNlp','Afinn','Vader','SentiStrength','Token Weight','Token Polarity','Sentence Polarity'])\n",
    "    return df\n",
    "\n",
    "for i in range(0,5):\n",
    "    df = dataframe_WordLevel(i)\n",
    "    df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Polarized/Modified Causal Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cg = nx.DiGraph()\n",
    "\n",
    "for ce in polarizedCauseffect:\n",
    "    s = str(ce[0])    \n",
    "    d = str(ce[2])\n",
    "    cg.add_node(s,id = str(s),title=str(s),x=615,y=200)\n",
    "    cg.add_node(d,id = str(d),title=str(d),x=615,y=200)\n",
    "    cg.add_edge(s, d, predicade=str(ce[1]))\n",
    "\n",
    "print(len(cg.nodes))\n",
    "print(len(cg.edges))\n",
    "\n",
    "pos = nx.spring_layout(cg, k=4, iterations=20)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "nx.draw(cg, pos=pos, with_labels=True,  node_shape=\"s\",  node_color=\"none\", font_size=15,  font_color='royalblue', font_weight='bold',bbox=dict(facecolor=\"white\", alpha=0.4,boxstyle='round,pad=0.8'),labels={node: node for node in cg.nodes()},arrows=True, arrowsize=15,width=1)\n",
    "edge_labels = nx.get_edge_attributes(cg,'predicade')\n",
    "nx.draw_networkx_edge_labels(cmap, pos=pos, edge_labels = edge_labels, font_color='black',font_size=13)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the Causal Graph on \"Causal Graph.json\" file for semi-automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_graph.node_link_data(cg)\n",
    "\n",
    "\n",
    "out_file = open(\"Causal Graph.json\", \"w\")\n",
    "  \n",
    "json.dump(data, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2bbca7b25589987bc69e95430193d6372f81759d34e6ca1eac7aad3047d77e5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
