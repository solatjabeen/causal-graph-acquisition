{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import textacy.preprocessing\n",
    "import textacy.resources\n",
    "#import textacy.keyterms\n",
    "import textacy.ke\n",
    "import neuralcoref\n",
    "from spacy.symbols import ORTH, POS, NOUN, VERB,PRON\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from networkx.readwrite import json_graph;\n",
    "import json\n",
    "from afinn import Afinn\n",
    "afn = Afinn()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from allennlp_models.pretrained import load_predictor\n",
    "predictor = load_predictor(\"roberta-sst\")\n",
    "from sentistrength import PySentiStr\n",
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath('C:/SentiStrengthCom.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('C:/SentStrength_Data_Sept2011/')\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import collections\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read text file and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Narrative:\n",
      "excessive overthinking leads to insomnia. stress caused insomnia. stress was caused by insomnia. insomnia is a result of stress. stress caused my insomnia. overthinking can increase anxiety and cause insomnia. my insomnia was caused by stress. stress is a reason of my insomnia. stress results to insomnia.\n",
      "Extracted Coreferences:\n",
      "[insomnia: [insomnia, insomnia, insomnia, insomnia, my insomnia, insomnia, my insomnia, my insomnia, insomnia]]\n",
      "\n",
      "\n",
      "Narrarive after resolving coreferences:\n",
      "excessive overthinking leads to insomnia. stress caused insomnia. stress was caused by insomnia. insomnia is a result of stress. stress caused insomnia. overthinking can increase anxiety and cause insomnia. insomnia was caused by stress. stress is a reason of insomnia. stress results to insomnia.\n",
      "\n",
      "Spacy Sentences:\n",
      "\n",
      "excessive overthinking leads to insomnia.\n",
      "stress caused insomnia.\n",
      "stress was caused by insomnia.\n",
      "insomnia is a result of stress.\n",
      "stress caused insomnia.\n",
      "overthinking can increase anxiety and cause insomnia.\n",
      "insomnia was caused by stress.\n",
      "stress is a reason of insomnia.\n",
      "stress results to insomnia.\n",
      "\n",
      "NLTK Sentences:\n",
      "\n",
      "excessive overthinking leads to insomnia.\n",
      "stress caused insomnia.\n",
      "stress was caused by insomnia.\n",
      "insomnia is a result of stress.\n",
      "stress caused insomnia.\n",
      "overthinking can increase anxiety and cause insomnia.\n",
      "insomnia was caused by stress.\n",
      "stress is a reason of insomnia.\n",
      "stress results to insomnia.\n"
     ]
    }
   ],
   "source": [
    "#from spacy.lang.en import English\n",
    "\n",
    "file_name = './Text/Medical Text.txt'\n",
    "#file_name = 'input.txt'\n",
    "narrative = open(file_name, encoding=\"utf8\").read()\n",
    "\n",
    "narrative = textacy.preprocessing.normalize_quotation_marks(narrative)\n",
    "#narrative = textacy.preprocessing.remove_punctuation(narrative, marks=\",;:\")\n",
    "#narrative = textacy.preprocessing.normalize_whitespace(narrative)\n",
    "#narrative = textacy.make_spacy_doc(narrative)\n",
    "narrative = narrative.lower()\n",
    "narrative = nlp(narrative)\n",
    "\n",
    "#narrative._.coref_clusters\n",
    "#narrative._.coref_resolved\n",
    "\n",
    "print('Original Narrative:')\n",
    "print(narrative)\n",
    "#print('\\n')\n",
    "print('Extracted Coreferences:')\n",
    "print(narrative._.coref_clusters)\n",
    "print('\\n')\n",
    "narrative = narrative._.coref_resolved\n",
    "print('Narrarive after resolving coreferences:')\n",
    "print(narrative)\n",
    "\n",
    "raw_sentences = sent_tokenize(narrative)\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "narrative = nlp(narrative)\n",
    "#sentences = [sent.string.strip() for sent in narrative.sents]\n",
    "#narrative = nlp(narrative)\n",
    "\n",
    "print('\\nSpacy Sentences:\\n')\n",
    "for sent in narrative.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "print('\\nNLTK Sentences:\\n')\n",
    "for sent in raw_sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Subject-Verb-Object triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sent in narrative.sents:\n",
    "    #print(\"Heyyyy!\")\n",
    "    print(sent)\n",
    "    for tok in sent:\n",
    "        print(tok.text,tok.pos_,tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      "excessive overthinking leads to insomnia.\n",
      "Noun Chunks:\n",
      "excessive overthinking\n",
      "insomnia\n",
      "Number of noun chunks:  2\n",
      "Method from textacy; subject_verb_object_triples extracted nothing!\n",
      "Triple by combining nsubj, root and dobj:\n",
      "(excessive overthinking, 'leads', insomnia)\n",
      "Sentence:\n",
      "stress caused insomnia.\n",
      "Noun Chunks:\n",
      "stress\n",
      "insomnia\n",
      "Number of noun chunks:  2\n",
      "Triples from textacy:\n",
      "('stress', 'caused', 'insomnia')\n",
      "Sentence:\n",
      "stress was caused by insomnia.\n",
      "Noun Chunks:\n",
      "stress\n",
      "insomnia\n",
      "Number of noun chunks:  2\n",
      "Method from textacy; subject_verb_object_triples extracted nothing!\n",
      "Triple by combining nsubj, root and dobj:\n",
      "(insomnia, 'caused', stress)\n",
      "Sentence:\n",
      "insomnia is a result of stress.\n",
      "Noun Chunks:\n",
      "insomnia\n",
      "a result\n",
      "stress\n",
      "Number of noun chunks:  3\n",
      "Method from textacy; subject_verb_object_triples extracted nothing!\n",
      "Triple by combining nsubj, root and dobj:\n",
      "(insomnia, 'is', stress)\n",
      "Triples other than sub, verb and obj:\n",
      "(insomnia, 'is', a result)\n",
      "(insomnia, 'is', stress)\n",
      "Sentence:\n",
      "stress caused insomnia.\n",
      "Noun Chunks:\n",
      "stress\n",
      "insomnia\n",
      "Number of noun chunks:  2\n",
      "Triples from textacy:\n",
      "('stress', 'caused', 'insomnia')\n",
      "Sentence:\n",
      "overthinking can increase anxiety and cause insomnia.\n",
      "Noun Chunks:\n",
      "overthinking\n",
      "anxiety\n",
      "insomnia\n",
      "Number of noun chunks:  3\n",
      "Triples from textacy:\n",
      "('overthinking', 'can increase', 'anxiety')\n",
      "Triples other than textacy:\n",
      "(overthinking, can increase, anxiety)\n",
      "(overthinking, can increase, insomnia)\n",
      "Sentence:\n",
      "insomnia was caused by stress.\n",
      "Noun Chunks:\n",
      "insomnia\n",
      "stress\n",
      "Number of noun chunks:  2\n",
      "Method from textacy; subject_verb_object_triples extracted nothing!\n",
      "Triple by combining nsubj, root and dobj:\n",
      "(stress, 'caused', insomnia)\n",
      "Sentence:\n",
      "stress is a reason of insomnia.\n",
      "Noun Chunks:\n",
      "stress\n",
      "a reason\n",
      "insomnia\n",
      "Number of noun chunks:  3\n",
      "Method from textacy; subject_verb_object_triples extracted nothing!\n",
      "Triple by combining nsubj, root and dobj:\n",
      "(stress, 'is', insomnia)\n",
      "Triples other than sub, verb and obj:\n",
      "(stress, 'is', a reason)\n",
      "(stress, 'is', insomnia)\n",
      "Sentence:\n",
      "stress results to insomnia.\n",
      "Noun Chunks:\n",
      "stress\n",
      "insomnia\n",
      "Number of noun chunks:  2\n",
      "Method from textacy; subject_verb_object_triples extracted nothing!\n",
      "Triple by combining nsubj, root and dobj:\n",
      "(stress, 'results', insomnia)\n"
     ]
    }
   ],
   "source": [
    "#new\n",
    "finalList = []\n",
    "sentences = []\n",
    "ncl = []\n",
    "nncl = [()]\n",
    "checkPass = False\n",
    "rootCheck = False\n",
    "varForm1 = False\n",
    "varForm2 = False\n",
    "#tuple =('a','b','c')\n",
    "\n",
    "#print(tuple[0])\n",
    "\n",
    "for sent in narrative.sents:\n",
    "    print(\"Sentence:\")\n",
    "    print(sent)\n",
    "    \n",
    "    print(\"Noun Chunks:\")\n",
    "    for nc in sent.noun_chunks:\n",
    "        print (nc)\n",
    "        ncl.append(nc)\n",
    "    print (\"Number of noun chunks: \", len(ncl))\n",
    "    \n",
    "    triplets = textacy.extract.subject_verb_object_triples(sent)\n",
    "    triplets = list(triplets)\n",
    "    if len(triplets) > 0:\n",
    "        print(\"Triples from textacy:\")\n",
    "        for t in triplets:\n",
    "            subject = str(t[0])\n",
    "            objec = str(t[2])\n",
    "            for chunk in ncl:\n",
    "                for cToken in chunk:\n",
    "                    if str(cToken) == str(t[0]):\n",
    "                        subject = str(chunk)\n",
    "                    if str(cToken) == str(t[2]):\n",
    "                        objec = str(chunk)\n",
    "            tup = (subject,str(t[1]),objec)\n",
    "            print(tup)\n",
    "            finalList.append(tup)\n",
    "            sentences.append(sent)\n",
    "        if len(ncl) == 3:\n",
    "            nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "            nncl.append((ncl[0],triplets[0][1],ncl[2]))\n",
    "            print(\"Triples other than textacy:\")\n",
    "            print(nncl[0])\n",
    "            print(nncl[1])\n",
    "            finalList.append(nncl[0])\n",
    "            sentences.append(sent)\n",
    "            finalList.append(nncl[1])\n",
    "            sentences.append(sent)\n",
    "#        elif len(ncl) == 2:\n",
    "#            nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "#            print(\"Triples other than textacy:\")\n",
    "#            print(nncl[0])\n",
    "#            finalList.append(nncl[0])\n",
    "    else:\n",
    "        print(\"Method from textacy; subject_verb_object_triples extracted nothing!\")\n",
    "        for token in sent:\n",
    "            #print(token.text, token.dep_,)\n",
    "            if token.dep_ == 'nsubj':\n",
    "                sub = token.text\n",
    "                #tuple[0] = str(token.text)\n",
    "            elif token.dep_ == 'nsubjpass':\n",
    "                checkPass = True\n",
    "                sub = token.text\n",
    "            elif token.dep_ == 'ROOT':\n",
    "                #print(\"Heyyyyyyy\")\n",
    "                verb = token.text\n",
    "                rootCheck = True\n",
    "            elif token.pos_ == 'NOUN' and token.dep_ == 'conj':\n",
    "                if rootCheck is True:\n",
    "                    varForm2 = True\n",
    "                else:\n",
    "                    varForm1 = True\n",
    "                #tuple[1] = token.text\n",
    "            #elif token.dep_ == 'dobj':\n",
    "            #    obj = token.text\n",
    "                #tuple[2] = token.text\n",
    "            #    break\n",
    "            else:\n",
    "                if token.dep_ == 'dobj':\n",
    "                    obj = token.text\n",
    "                    continue\n",
    "                elif token.dep_ == 'pobj':\n",
    "                    obj = token.text\n",
    "                    continue\n",
    "                \n",
    "        for nChunk in ncl:\n",
    "            for nToken in nChunk:\n",
    "                if str(nToken) == str(sub):\n",
    "                    sub = nChunk\n",
    "                if str(nToken) == str(obj):\n",
    "                    obj = nChunk\n",
    "        if checkPass is True:\n",
    "            tuple = (obj,verb,sub)\n",
    "        else:\n",
    "            tuple = (sub,verb,obj)\n",
    "        print(\"Triple by combining nsubj, root and dobj:\")\n",
    "        print(tuple)\n",
    "        finalList.append(tuple)\n",
    "        sentences.append(sent)\n",
    "        if len(ncl) == 3:\n",
    "            if varForm1 == True:\n",
    "                nncl = [(ncl[0],verb,ncl[2])]\n",
    "                nncl.append((ncl[1],verb,ncl[2]))\n",
    "                print(\"Triples other than sub, verb and obj:\")\n",
    "                print(nncl[0])\n",
    "                print(nncl[1])\n",
    "                finalList.append(nncl[0])\n",
    "                sentences.append(sent)\n",
    "                finalList.append(nncl[1])\n",
    "                sentences.append(sent)\n",
    "            else:\n",
    "                nncl = [(ncl[0],verb,ncl[1])]\n",
    "                nncl.append((ncl[0],verb,ncl[2]))\n",
    "                print(\"Triples other than sub, verb and obj:\")\n",
    "                print(nncl[0])\n",
    "                print(nncl[1])\n",
    "                finalList.append(nncl[0])\n",
    "                sentences.append(sent)\n",
    "                finalList.append(nncl[1])\n",
    "                sentences.append(sent)\n",
    "            \n",
    "#        elif len(ncl) == 2:\n",
    "#            nncl = [(ncl[0],verb,ncl[1])]\n",
    "#            print(\"Triples other than sub, verb and obj:\")\n",
    "#            print(nncl[0])\n",
    "#            finalList.append(nncl[0])\n",
    "\n",
    "    ncl.clear()\n",
    "    nncl.clear()\n",
    "    rootCheck = False\n",
    "    checkPass = False\n",
    "    varForm1 = False\n",
    "    varForm2 = False\n",
    "    \n",
    "trips = finalList\n",
    "\n",
    "#        for token in nc:\n",
    "#            print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#old\n",
    "def SVO(narrative):\n",
    "    ncl = []\n",
    "    fl = []\n",
    "    nncl = [()]\n",
    "    for sent in narrative.sents:\n",
    "        #sent = str(sent)\n",
    "        print(\"\\n\" + str(sent))\n",
    "        for nc in sent.noun_chunks:\n",
    "            print (nc)\n",
    "            ncl.append(nc)\n",
    "        print(len(ncl))       \n",
    "        triplets = textacy.extract.subject_verb_object_triples(sent)\n",
    "        triplets = list(triplets)\n",
    "        if len(ncl) <= 3:\n",
    "            if len(triplets) == 0:\n",
    "                print(\"Method from textacy; subject_verb_object_triples extracted nothing!\")\n",
    "                for token in sent:\n",
    "                #print(token.text, token.dep_,)\n",
    "                    if token.dep_ == 'ROOT':\n",
    "                        text = token.text\n",
    "                        break\n",
    "                    #print(token.text)\n",
    "                if len(ncl) == 3:\n",
    "                        nncl = [(ncl[0],token.text,ncl[1])]\n",
    "                        nncl.append((ncl[0],token.text,ncl[2]))\n",
    "                        fl.append(nncl[0])\n",
    "                        fl.append(nncl[1])\n",
    "                elif len(ncl) == 2:\n",
    "                        nncl = [(ncl[0],token.text,ncl[1])]\n",
    "                        fl.append(nncl[0])\n",
    "                else:\n",
    "                    continue\n",
    "                print(\"Triples without textacy.\")\n",
    "                print(nncl)\n",
    "        \n",
    "            else:\n",
    "                print(triplets)\n",
    "                for t in triplets:\n",
    "                    print(t)\n",
    "                    fl.append(t)\n",
    "                if len(ncl) == 3:\n",
    "                    nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "                    nncl.append((ncl[0],triplets[0][1],ncl[2]))\n",
    "                    fl.append(nncl[0])\n",
    "                    fl.append(nncl[1])\n",
    "                    #print(nncl[0])\n",
    "                    #print(nncl[1])\n",
    "                elif len(ncl) == 2:\n",
    "                    nncl = [(ncl[0],triplets[0][1],ncl[1])]\n",
    "                    fl.append(nncl[0])\n",
    "                else:\n",
    "                    continue;\n",
    "                print(nncl)\n",
    "        ncl.clear()\n",
    "        nncl.clear()\n",
    "    return fl\n",
    "\n",
    "trips = SVO(narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the extracted triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, triple  in zip(sentences, trips):\n",
    "    print(sentence, triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap = nx.DiGraph()\n",
    "\n",
    "for triple in trips:\n",
    "    s = str(triple[0])    \n",
    "    d = str(triple[2])\n",
    "    cmap.add_edge(s, d, predicade=triple[1])\n",
    "\n",
    "print(len(cmap.nodes))\n",
    "print(len(cmap.edges))\n",
    "\n",
    "pos = nx.spring_layout(cmap, k=4, iterations=20)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "nx.draw(cmap, pos=pos, with_labels=True,  node_shape=\"s\",  node_color=\"none\", font_size=15,  font_color='royalblue', font_weight='bold',bbox=dict(facecolor=\"white\", alpha=0.4,boxstyle='round,pad=0.8'),labels={node: node for node in cmap.nodes()},arrows=True, arrowsize=15,width=1)\n",
    "edge_labels = nx.get_edge_attributes(cmap,'predicade')\n",
    "nx.draw_networkx_edge_labels(cmap, pos=pos, edge_labels = edge_labels, font_color='black',font_size=13)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the Knowledge Graph on \"Knowledge Graph.json\" file for                           semi-automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgeGraph = nx.DiGraph()\n",
    "\n",
    "for svo in trips:\n",
    "    s = str(svo[0])    \n",
    "    d = str(svo[2])\n",
    "    knowledgeGraph.add_node(s,id = str(s),title=str(s),x=615,y=200)\n",
    "    knowledgeGraph.add_node(d,id = str(d),title=str(d),x=615,y=200)\n",
    "    knowledgeGraph.add_edge(s, d, predicade=str(svo[1]))\n",
    "\n",
    "#print(len(knowledgeGraph.nodes))\n",
    "#print(len(knowledgeGraph.edges))\n",
    "\n",
    "data = json_graph.node_link_data(knowledgeGraph)\n",
    "\n",
    "\n",
    "out_file = open(\"Knowledge Graph.json\", \"w\")\n",
    "  \n",
    "json.dump(data, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Causal Trigger Words Dictionary along with their synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27499\n"
     ]
    }
   ],
   "source": [
    "#new\n",
    "\n",
    "causalWords1 = [\"forced\",\"caused\", \"resulted\", \"reason\", \"as a result of\", \"as a consequence of\", \n",
    "               \"consequence\", \"consequently\", \"affect\", \"because\", \"increase\", \"decrease\",\"due to\",\"because of\"\n",
    "               ,\"made\",\"mimimize\",\"maximize\",\"hindered\", \"displaced\", \"conspired\",\"led to\",\"activate\",\"impel\",\"inspire\",\n",
    "                \"excite\",\"quicken\",\"rouse\",\"stimulate\",\"influence\",\"determine\",\"likely\",\"probable\",\"disconnected\",\"separated\"\n",
    "                ,\"excluded\",\"after\",\"as\",\"since\",\"trigger\"]\n",
    "\n",
    "#causalWords1 = [\"forced\"]\n",
    "synonyms1 = []\n",
    "lemma_function = WordNetLemmatizer()\n",
    "\n",
    "for cw1 in causalWords1:\n",
    "    #print(cw1)\n",
    "    synonyms1.append(cw1.lower())\n",
    "    tokens1 = word_tokenize(cw1)\n",
    "    for token1, tag1 in pos_tag(tokens1):\n",
    "        lemma1 = lemma_function.lemmatize(token1)\n",
    "        #print(token1,lemma1)\n",
    "        \n",
    "        for syn in wordnet.synsets(str(lemma1)):\n",
    "            \n",
    "            for l1 in syn.lemmas():\n",
    "                #print(l1)\n",
    "                #synonyms1.append(lemma1.lower())\n",
    "                synonyms1.append(l1.name().lower())\n",
    "                #print(l1)\n",
    "                for syn1 in wordnet.synsets(str(l1.name())):\n",
    "                    for l2 in syn1.lemmas():\n",
    "                        #print(l2)\n",
    "                        synonyms1.append(l2.name().lower())\n",
    "            \n",
    "print(len(synonyms1))\n",
    "#print(set(synonyms1))\n",
    "#for synonym in synonyms1:\n",
    "#    print(synonym+\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "\n",
    "causalWords = [\"forced\",\"caused\", \"result\", \"reason\", \"as a result of\", \"as a consequence of\", \n",
    "               \"consequence\", \"consequently\", \"affect\", \"because\", \"increase\", \"decrease\",\"due to\",\"because of\"\n",
    "               ,\"made\",\"mimimize\",\"maximize\",\"led to\",\"produced\"]\n",
    "\n",
    "#causalWords = [\"as a result of\"]\n",
    "synonyms = []\n",
    "\n",
    "for cw in causalWords:\n",
    "    \n",
    "    for syn in wordnet.synsets(str(cw)):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(cw)\n",
    "            synonyms.append(l.name())\n",
    "        \n",
    "print(set(synonyms))\n",
    "print(len(synonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Causal triples from SVO triples based upon causal trigger                           words dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excessive overthinking leads to insomnia. (excessive overthinking, 'leads', insomnia)\n",
      "stress caused insomnia. ('stress', 'caused', 'insomnia')\n",
      "stress was caused by insomnia. (insomnia, 'caused', stress)\n",
      "stress caused insomnia. ('stress', 'caused', 'insomnia')\n",
      "overthinking can increase anxiety and cause insomnia. ('overthinking', 'can increase', 'anxiety')\n",
      "overthinking can increase anxiety and cause insomnia. (overthinking, can increase, anxiety)\n",
      "overthinking can increase anxiety and cause insomnia. (overthinking, can increase, insomnia)\n",
      "insomnia was caused by stress. (stress, 'caused', insomnia)\n",
      "stress results to insomnia. (stress, 'results', insomnia)\n"
     ]
    }
   ],
   "source": [
    "causeffect = []\n",
    "causalSentences = []\n",
    "#lem = []\n",
    "#lemma_function = WordNetLemmatizer()\n",
    "for s,st in zip(sentences,trips):\n",
    "    #st = (str(st[0]),str(st[1]),str(st[2]))\n",
    "    #synonym = synonym[0]\n",
    "    tokens = word_tokenize(str(st[1]))\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemma_function.lemmatize(token)\n",
    "        #print(token,lemma)\n",
    "        #st[1] = lemma\n",
    "        #lem.append(lemma)\n",
    "        \n",
    "    for synonym in synonyms1:\n",
    "        if synonym == lemma:\n",
    "            #causeffect = [(st[0],st[1],st[2])]\n",
    "            #strTriples.append((ncl[0],token.text,st[2]))\n",
    "           # print(st)\n",
    "            causeffect.append(st)\n",
    "            causalSentences.append(s)\n",
    "            #if (len(causeffect) >0 ) and (st in causeffect == False):\n",
    "                \n",
    "            #    causeffect.append(st)\n",
    "            break\n",
    "    \n",
    "#causeffect.sort(key = lambda x: x[0])\n",
    "\n",
    "#for sortedTriple in causeffect:\n",
    "#    print(sortedTriple)\n",
    "\n",
    "for causalSentence, causalTriple  in zip(causalSentences, causeffect):\n",
    "    print(causalSentence, causalTriple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Causal Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = nx.DiGraph()\n",
    "\n",
    "for ce in causeffect:\n",
    "    s = str(ce[0])    \n",
    "    d = str(ce[2])\n",
    "    cg.add_node(s,id = str(s),title=str(s),x=615,y=200)\n",
    "    cg.add_node(d,id = str(d),title=str(d),x=615,y=200)\n",
    "    cg.add_edge(s, d, predicade=str(ce[1]))\n",
    "\n",
    "print(len(cg.nodes))\n",
    "print(len(cg.edges))\n",
    "\n",
    "pos = nx.spring_layout(cg, k=4, iterations=20)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "nx.draw(cg, pos=pos, with_labels=True,  node_shape=\"s\",  node_color=\"none\", font_size=15,  font_color='royalblue', font_weight='bold',bbox=dict(facecolor=\"white\", alpha=0.4,boxstyle='round,pad=0.8'),labels={node: node for node in cg.nodes()},arrows=True, arrowsize=15,width=1)\n",
    "edge_labels = nx.get_edge_attributes(cg,'predicade')\n",
    "nx.draw_networkx_edge_labels(cmap, pos=pos, edge_labels = edge_labels, font_color='black',font_size=13)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates from causalSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[excessive overthinking leads to insomnia.,\n",
       " stress caused insomnia.,\n",
       " stress was caused by insomnia.,\n",
       " stress caused insomnia.,\n",
       " overthinking can increase anxiety and cause insomnia.,\n",
       " insomnia was caused by stress.,\n",
       " stress results to insomnia.]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causalSentences = list(dict.fromkeys(causalSentences))\n",
    "#causalSentences = list(set(causalSentences))\n",
    "\n",
    "#causalSentences1 = []\n",
    "#for c in causalSentences:\n",
    "#    if c not in causalSentences1:\n",
    "#        #print(c)\n",
    "#        causalSentences1.append(c)\n",
    "\n",
    "causalSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Sentences' Polarity (Sentence Level) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AllenNlp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excessive overthinking leads to insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "stress was caused by insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "overthinking can increase anxiety and cause insomnia. -1\n",
      "insomnia was caused by stress. -1\n",
      "stress results to insomnia. -1\n"
     ]
    }
   ],
   "source": [
    "#https://demo.allennlp.org/sentiment-analysis/roberta-sentiment-analysis\n",
    "#https://paperswithcode.com/model/roberta-large-sst\n",
    "\n",
    "#output (Pos,neg)\n",
    "\n",
    "#sentence = \"Enemies surround Pakistan.\"\n",
    "#preds = predictor.predict(sentence)\n",
    "#print(f\"p(positive)={preds['probs'][0]:.2%}\")\n",
    "allen = []\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    preds = predictor.predict(str(sent))\n",
    "    #print(preds['label'][0])\n",
    "    label = preds['label'][0]\n",
    "    if label is '0':\n",
    "        allen.append(-1)\n",
    "    else:\n",
    "        allen.append(1)\n",
    "    \n",
    "for causalSentence, polarity  in zip(causalSentences, allen):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Afinn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excessive overthinking leads to insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "stress was caused by insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "overthinking can increase anxiety and cause insomnia. -1\n",
      "insomnia was caused by stress. -1\n",
      "stress results to insomnia. -1\n"
     ]
    }
   ],
   "source": [
    "#https://www.geeksforgeeks.org/python-sentiment-analysis-using-affin/\n",
    "\n",
    "#output (Pos,neg,neural)\n",
    "\n",
    "afinn = []\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    score = afn.score(str(sent))\n",
    "    #print(score)\n",
    "    if score > 0:\n",
    "        #print('positive')\n",
    "        afinn.append(1)\n",
    "    elif score < 0:\n",
    "        #print('negative')\n",
    "        afinn.append(-1)\n",
    "    else:\n",
    "        #print('neutral')\n",
    "        afinn.append(0)\n",
    "        \n",
    "for causalSentence, polarity  in zip(causalSentences, afinn):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excessive overthinking leads to insomnia. 0\n",
      "stress caused insomnia. -1\n",
      "stress was caused by insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "overthinking can increase anxiety and cause insomnia. 1\n",
      "insomnia was caused by stress. -1\n",
      "stress results to insomnia. -1\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/cjhutto/vaderSentiment#about-the-scoring\n",
    "#output (Pos,neg,neural)\n",
    "\n",
    "vader = []\n",
    "\n",
    "def vadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    #print(vs)\n",
    "    return vs['compound']\n",
    "\n",
    "def vader_analysis(compound):\n",
    "    if compound >= 0.05:\n",
    "        vader.append(1)\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.05 :\n",
    "        vader.append(-1)\n",
    "        return 'Negative'\n",
    "        \n",
    "    elif  compound > -0.05 and compound < 0.05:\n",
    "        vader.append(0)\n",
    "        return 'Neutral'\n",
    "    \n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    compound = vadersentimentanalysis(str(sent))\n",
    "    vader_analysis(compound)\n",
    "    \n",
    "for causalSentence, polarity  in zip(causalSentences, vader):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiStrength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excessive overthinking leads to insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "stress was caused by insomnia. -1\n",
      "stress caused insomnia. -1\n",
      "overthinking can increase anxiety and cause insomnia. -1\n",
      "insomnia was caused by stress. -1\n",
      "stress results to insomnia. -1\n"
     ]
    }
   ],
   "source": [
    "#http://sentistrength.wlv.ac.uk/\n",
    "#http://paper.ijcsns.org/07_book/202001/20200107.pdf\n",
    "#https://pypi.org/project/sentistrength/\n",
    "#http://sentistrength.wlv.ac.uk/results.php?text=pakistan+must+support+army+and+aq+khan.&submit=Detect+Sentiment&result=trinary\n",
    "#https://professorkhan.com/2019/03/29/sentiment-analysis-with-sentistrength/\n",
    "#output (Pos,neg)\n",
    "\n",
    "sentiStrength = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    result = senti.getSentiment(str(sent), score='binary')\n",
    "    #print(result)\n",
    "    \n",
    "    if result[0]==1:\n",
    "        #print('Positive')\n",
    "        sentiStrength.append(1)\n",
    "    elif result[0]==-1:\n",
    "        sentiStrength.append(-1)\n",
    "        #print('Negative')\n",
    "    #else:\n",
    "    #    print('Neutral')\n",
    "\n",
    "for causalSentence, polarity  in zip(causalSentences, sentiStrength):\n",
    "    print(causalSentence, polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiWordnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def sentiment_sentiwordnet(text):\n",
    "    #text = text.decode(\"utf-8\")\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    #print(raw_sentences)\n",
    "    sentiment = 0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        print(raw_sentence)\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        print(tagged_sentence)\n",
    "        \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "                \n",
    "            #lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            #if not lemma:\n",
    "            #    continue\n",
    "                \n",
    "            synsets = wn.synsets(word, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "                \n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "        # print(swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score())\n",
    "        if not tokens_count:\n",
    "            return 0\n",
    "        if sentiment>0:\n",
    "            #return \"Positive\"\n",
    "            print(\"Positive\")\n",
    "        elif sentiment==0:\n",
    "            #return \"Neutral\"\n",
    "            print(\"Neutral\")\n",
    "        else:\n",
    "            #return \"Negative\"\n",
    "            print(\"Negative\")\n",
    "    \n",
    "causalNarrative = \"\"\n",
    "for cS in causalSentences:\n",
    "    causalNarrative = causalNarrative+str(cS)+\" \"\n",
    "\n",
    "causalNarrative = nlp(causalNarrative)\n",
    "#raw_sentences = sent_tokenize(causalNarrative)\n",
    "#print(str(causalNarrative)+\"\\n\")\n",
    "#print(raw_sentences) \n",
    "#print(str(narrative))\n",
    "\n",
    "sentiment_sentiwordnet(str(narrative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiWordNet OnlyVerbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "    \n",
    "\n",
    "polarizedCauseffect = []\n",
    "for ce in causeffect:\n",
    "    print(ce[1])\n",
    "    token = nltk.word_tokenize(str(ce[1]))\n",
    "    after_tagging = nltk.pos_tag(token)\n",
    "    print(after_tagging)\n",
    "    print(len(after_tagging))\n",
    "    for tag in after_tagging:\n",
    "        if tag[1].startswith('V') or len(after_tagging)==1:\n",
    "            words = swn.senti_synsets(tag[0]) \n",
    "            answer = list(words)[0]\n",
    "            if (answer.pos_score() > answer.neg_score()):\n",
    "                mce = str(ce[1])+\"(+ve)\"\n",
    "                ce1=(ce[0],mce,ce[2])\n",
    "                polarizedCauseffect.append(ce1)\n",
    "                print(ce1)\n",
    "            elif (answer.pos_score() > 0):\n",
    "                mce = str(ce[1])+\"(-ve)\"\n",
    "                ce1=(ce[0],mce,ce[2])\n",
    "                polarizedCauseffect.append(ce1)\n",
    "                print(ce1)\n",
    "            else: \n",
    "                mce = str(ce[1])+\"(neutral)\"\n",
    "                ce1=(ce[0],mce,ce[2])\n",
    "                polarizedCauseffect.append(ce1)\n",
    "                print(ce1)\n",
    "                \n",
    "polarizedCauseffect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame (Sentence Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Causal Sentence</th>        <th class=\"col_heading level0 col1\" >AllenNLP</th>        <th class=\"col_heading level0 col2\" >Afinn</th>        <th class=\"col_heading level0 col3\" >Vader</th>        <th class=\"col_heading level0 col4\" >SentiStrength</th>        <th class=\"col_heading level0 col5\" >Weight</th>        <th class=\"col_heading level0 col6\" >Polarity</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col0\" class=\"data row0 col0\" >excessive overthinking leads to insomnia.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col1\" class=\"data row0 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col2\" class=\"data row0 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col4\" class=\"data row0 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col5\" class=\"data row0 col5\" >-3</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row0_col6\" class=\"data row0 col6\" >Moderate Negative</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col0\" class=\"data row1 col0\" >stress caused insomnia.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col1\" class=\"data row1 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col2\" class=\"data row1 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col3\" class=\"data row1 col3\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col4\" class=\"data row1 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col5\" class=\"data row1 col5\" >-4</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row1_col6\" class=\"data row1 col6\" >Strong Negative</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col0\" class=\"data row2 col0\" >stress was caused by insomnia.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col1\" class=\"data row2 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col2\" class=\"data row2 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col3\" class=\"data row2 col3\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col4\" class=\"data row2 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col5\" class=\"data row2 col5\" >-4</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row2_col6\" class=\"data row2 col6\" >Strong Negative</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col0\" class=\"data row3 col0\" >stress caused insomnia.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col1\" class=\"data row3 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col2\" class=\"data row3 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col3\" class=\"data row3 col3\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col4\" class=\"data row3 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col5\" class=\"data row3 col5\" >-4</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row3_col6\" class=\"data row3 col6\" >Strong Negative</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col0\" class=\"data row4 col0\" >overthinking can increase anxiety and cause insomnia.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col1\" class=\"data row4 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col2\" class=\"data row4 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col4\" class=\"data row4 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col5\" class=\"data row4 col5\" >-2</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row4_col6\" class=\"data row4 col6\" >Mild Negative</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col0\" class=\"data row5 col0\" >insomnia was caused by stress.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col1\" class=\"data row5 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col2\" class=\"data row5 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col3\" class=\"data row5 col3\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col4\" class=\"data row5 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col5\" class=\"data row5 col5\" >-4</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row5_col6\" class=\"data row5 col6\" >Strong Negative</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col0\" class=\"data row6 col0\" >stress results to insomnia.</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col1\" class=\"data row6 col1\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col2\" class=\"data row6 col2\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col3\" class=\"data row6 col3\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col4\" class=\"data row6 col4\" >-1</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col5\" class=\"data row6 col5\" >-4</td>\n",
       "                        <td id=\"T_90c3f6c2_3519_11ec_83d5_648099c56c92row6_col6\" class=\"data row6 col6\" >Strong Negative</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x249d90ecd08>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df2 = pd.DataFrame([[38.0, 2.0, 18.0, 22.0, 21, 0,0],[19, 439, 6, 452, 226,232,0]],columns=['Causal Sentence','AllenNlp','Afinn','Vader','SentiStrength','Weight','Polarity'])\n",
    "\n",
    "df2 = pd.DataFrame(list(zip(causalSentences, allen, afinn, vader,sentiStrength)), columns =['Causal Sentence', 'AllenNLP','Afinn','Vader','SentiStrength'])\n",
    "df2[\"Weight\"] = df2.sum(axis=1)\n",
    "\n",
    "\n",
    "sentPolarity = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    weight = row['Weight']\n",
    "    #print(weight)\n",
    "    if weight is -4:\n",
    "        sentPolarity.append('Strong Negative') \n",
    "        #print('Strong Negative')\n",
    "    elif weight is -3:\n",
    "        sentPolarity.append('Moderate Negative')\n",
    "        #print('Moderate Negative')\n",
    "    elif weight is -2:\n",
    "        sentPolarity.append('Mild Negative')\n",
    "        #print('Mild Negative')\n",
    "    elif weight is -1:\n",
    "        sentPolarity.append('Weak Negative')\n",
    "        #print('Weak Negative')\n",
    "    elif weight is 0:\n",
    "        sentPolarity.append('Neutral')\n",
    "        #print('Neutral')\n",
    "    elif weight is 4:\n",
    "        sentPolarity.append('Strong Positive')\n",
    "        #print('Strong Positive')\n",
    "    elif weight is 3:\n",
    "        sentPolarity.append('Moderate Positive')\n",
    "        #print('Moderate Positive')\n",
    "    elif weight is 2:\n",
    "        sentPolarity.append('Mild Positive')\n",
    "        #print('Mild Positive')\n",
    "    elif weight is 1:\n",
    "        sentPolarity.append('Weak Positive')\n",
    "        #print('Weak Positive')\n",
    "\n",
    "df2[\"Polarity\"] = sentPolarity\n",
    "df2.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Sentences' Polarity (Word Level) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating list of tokens without punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[['excessive', 'overthinking', 'leads', 'to', 'insomnia'], ['stress', 'caused', 'insomnia'], ['stress', 'was', 'caused', 'by', 'insomnia'], ['stress', 'caused', 'insomnia'], ['overthinking', 'can', 'increase', 'anxiety', 'and', 'cause', 'insomnia'], ['insomnia', 'was', 'caused', 'by', 'stress'], ['stress', 'results', 'to', 'insomnia']]\n"
     ]
    }
   ],
   "source": [
    "marks = ['.',',','(',')','-',':',';','/',\"'s\",'_','__','--','\"']\n",
    "token_list = []\n",
    "token_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    for token in sent:\n",
    "        if str(token) not in marks:\n",
    "            token_tuple.append(str(token))\n",
    "    token_list.append(token_tuple)\n",
    "    token_tuple = []\n",
    "    \n",
    "print(len(token_list))\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AllenNlp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[-1, -1, 1, 1, -1]\n",
      "3\n",
      "[-1, 1, -1]\n",
      "5\n",
      "[-1, 1, 1, -1, -1]\n",
      "3\n",
      "[-1, 1, -1]\n",
      "7\n",
      "[-1, 1, 1, -1, 1, 1, -1]\n",
      "5\n",
      "[-1, 1, 1, -1, -1]\n",
      "4\n",
      "[-1, 1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "allen_word = []\n",
    "allen_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) not in marks: \n",
    "            #print(token)\n",
    "            preds = predictor.predict(str(token))\n",
    "            #print(preds['label'][0])\n",
    "            label = preds['label'][0]\n",
    "            if label is '0':\n",
    "                allen_tuple.append(-1)\n",
    "                #allen_word.append(-1)\n",
    "            else:\n",
    "                #allen_word.append(1)\n",
    "                allen_tuple.append(1)\n",
    "    allen_word.append(allen_tuple)\n",
    "    allen_tuple = []\n",
    "                \n",
    "for a in allen_word:\n",
    "    print(len(a))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Afinn] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0, 0, 0, 0, -1]\n",
      "3\n",
      "[-1, 0, -1]\n",
      "5\n",
      "[-1, 0, 0, 0, -1]\n",
      "3\n",
      "[-1, 0, -1]\n",
      "7\n",
      "[0, 0, 1, -1, 0, 0, -1]\n",
      "5\n",
      "[-1, 0, 0, 0, -1]\n",
      "4\n",
      "[-1, 0, 0, -1]\n"
     ]
    }
   ],
   "source": [
    "afinn_word = []\n",
    "afinn_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        #if str(token) != \".\": #or str(token) != ',' or str(token) != \"-\":\n",
    "        if str(token) not in marks:\n",
    "            #print(token)\n",
    "            score = afn.score(str(token))\n",
    "            if score > 0:\n",
    "                #print('positive')\n",
    "                afinn_tuple.append(1)\n",
    "            elif score < 0:\n",
    "                #print('negative')\n",
    "                afinn_tuple.append(-1)\n",
    "            else:\n",
    "                #print('neutral')\n",
    "                afinn_tuple.append(0)\n",
    "    afinn_word.append(afinn_tuple)\n",
    "    afinn_tuple = []\n",
    "\n",
    "for a in afinn_word:\n",
    "    print(len(a))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vader] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0, 0, 0, 0, 0]\n",
      "3\n",
      "[-1, 0, 0]\n",
      "5\n",
      "[-1, 0, 0, 0, 0]\n",
      "3\n",
      "[-1, 0, 0]\n",
      "7\n",
      "[0, 0, 1, -1, 0, 0, 0]\n",
      "5\n",
      "[0, 0, 0, 0, -1]\n",
      "4\n",
      "[-1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vader_word = []\n",
    "vader_tuple = []\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) not in marks:\n",
    "            vs = analyzer.polarity_scores(str(token))\n",
    "            compound = vs['compound']\n",
    "            if compound >= 0.05:\n",
    "                vader_tuple.append(1)   \n",
    "            elif compound <= -0.05 :\n",
    "                vader_tuple.append(-1)\n",
    "            elif  compound > -0.05 and compound < 0.05:\n",
    "                vader_tuple.append(0)\n",
    "    vader_word.append(vader_tuple)\n",
    "    vader_tuple = []\n",
    "\n",
    "for a in vader_word:\n",
    "    print(len(a))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SentiStrength] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[-1, 1, 1, 1, 1]\n",
      "3\n",
      "[-1, 1, 1]\n",
      "5\n",
      "[-1, 1, 1, 1, 1]\n",
      "3\n",
      "[-1, 1, 1]\n",
      "7\n",
      "[1, 1, 1, -1, 1, 1, 1]\n",
      "5\n",
      "[1, 1, 1, 1, -1]\n",
      "4\n",
      "[-1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "senti_word = []\n",
    "senti_tuple = []\n",
    "\n",
    "\n",
    "for sent in causalSentences:\n",
    "    #print(sent)\n",
    "    for token in sent:\n",
    "        if str(token) not in marks:\n",
    "            #print(token)\n",
    "            result = senti.getSentiment(str(token), score='binary')\n",
    "            #print(result)\n",
    "            if result[0]==1:\n",
    "                #print('Positive')\n",
    "                senti_tuple.append(1)\n",
    "            elif result[0]==-1:\n",
    "                #print('Negative')\n",
    "                senti_tuple.append(-1)\n",
    "            else:\n",
    "                #print('Neutral')\n",
    "                senti_tuple.append(0)\n",
    "    senti_word.append(senti_tuple)\n",
    "    senti_tuple = []\n",
    "    \n",
    "for a in senti_word:\n",
    "    print(len(a))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame (Word Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222982c9cdbb4999a40fd0b18e674043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>excessive</td>\n",
       "      <td>overthinking</td>\n",
       "      <td>leads</td>\n",
       "      <td>to</td>\n",
       "      <td>insomnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Mild Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Weak Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0             1              2              3  \\\n",
       "Tokens                 excessive  overthinking          leads             to   \n",
       "AllenNlp                      -1            -1              1              1   \n",
       "Afinn                          0             0              0              0   \n",
       "Vader                          0             0              0              0   \n",
       "SentiStrength                 -1             1              1              1   \n",
       "Token Weight                  -2             0              2              2   \n",
       "Token Polarity     Mild Negative       Neutral  Mild Positive  Mild Positive   \n",
       "Sentence Polarity  Mild Positive          None           None           None   \n",
       "\n",
       "                               4  \n",
       "Tokens                  insomnia  \n",
       "AllenNlp                      -1  \n",
       "Afinn                         -1  \n",
       "Vader                          0  \n",
       "SentiStrength                  1  \n",
       "Token Weight                  -1  \n",
       "Token Polarity     Weak Negative  \n",
       "Sentence Polarity           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63001704334c4c7888c5424da621e650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>stress</td>\n",
       "      <td>caused</td>\n",
       "      <td>insomnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Weak Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0              1              2\n",
       "Tokens                      stress         caused       insomnia\n",
       "AllenNlp                        -1              1             -1\n",
       "Afinn                           -1              0             -1\n",
       "Vader                           -1              0              0\n",
       "SentiStrength                   -1              1              1\n",
       "Token Weight                    -4              2             -1\n",
       "Token Polarity     Strong Negative  Mild Positive  Weak Negative\n",
       "Sentence Polarity  Strong Negative           None           None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a626971faa7348a1a2c22c0f712c7ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>stress</td>\n",
       "      <td>was</td>\n",
       "      <td>caused</td>\n",
       "      <td>by</td>\n",
       "      <td>insomnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Weak Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0              1              2        3  \\\n",
       "Tokens                      stress            was         caused       by   \n",
       "AllenNlp                        -1              1              1       -1   \n",
       "Afinn                           -1              0              0        0   \n",
       "Vader                           -1              0              0        0   \n",
       "SentiStrength                   -1              1              1        1   \n",
       "Token Weight                    -4              2              2        0   \n",
       "Token Polarity     Strong Negative  Mild Positive  Mild Positive  Neutral   \n",
       "Sentence Polarity    Mild Positive           None           None     None   \n",
       "\n",
       "                               4  \n",
       "Tokens                  insomnia  \n",
       "AllenNlp                      -1  \n",
       "Afinn                         -1  \n",
       "Vader                          0  \n",
       "SentiStrength                  1  \n",
       "Token Weight                  -1  \n",
       "Token Polarity     Weak Negative  \n",
       "Sentence Polarity           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a975d8424b4c0c9b83a95f85174bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>stress</td>\n",
       "      <td>caused</td>\n",
       "      <td>insomnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Weak Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0              1              2\n",
       "Tokens                      stress         caused       insomnia\n",
       "AllenNlp                        -1              1             -1\n",
       "Afinn                           -1              0             -1\n",
       "Vader                           -1              0              0\n",
       "SentiStrength                   -1              1              1\n",
       "Token Weight                    -4              2             -1\n",
       "Token Polarity     Strong Negative  Mild Positive  Weak Negative\n",
       "Sentence Polarity  Strong Negative           None           None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73413145cb27448891478f58f1e7f795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>overthinking</td>\n",
       "      <td>can</td>\n",
       "      <td>increase</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>and</td>\n",
       "      <td>cause</td>\n",
       "      <td>insomnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Strong Positive</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Weak Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0              1                2  \\\n",
       "Tokens              overthinking            can         increase   \n",
       "AllenNlp                      -1              1                1   \n",
       "Afinn                          0              0                1   \n",
       "Vader                          0              0                1   \n",
       "SentiStrength                  1              1                1   \n",
       "Token Weight                   0              2                4   \n",
       "Token Polarity           Neutral  Mild Positive  Strong Positive   \n",
       "Sentence Polarity  Mild Positive           None             None   \n",
       "\n",
       "                                 3              4              5  \\\n",
       "Tokens                     anxiety            and          cause   \n",
       "AllenNlp                        -1              1              1   \n",
       "Afinn                           -1              0              0   \n",
       "Vader                           -1              0              0   \n",
       "SentiStrength                   -1              1              1   \n",
       "Token Weight                    -4              2              2   \n",
       "Token Polarity     Strong Negative  Mild Positive  Mild Positive   \n",
       "Sentence Polarity             None           None           None   \n",
       "\n",
       "                               6  \n",
       "Tokens                  insomnia  \n",
       "AllenNlp                      -1  \n",
       "Afinn                         -1  \n",
       "Vader                          0  \n",
       "SentiStrength                  1  \n",
       "Token Weight                  -1  \n",
       "Token Polarity     Weak Negative  \n",
       "Sentence Polarity           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848d4e7adb604c1abc58eaff3f3d184f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>insomnia</td>\n",
       "      <td>was</td>\n",
       "      <td>caused</td>\n",
       "      <td>by</td>\n",
       "      <td>stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Weak Negative</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Strong Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0              1              2        3  \\\n",
       "Tokens                  insomnia            was         caused       by   \n",
       "AllenNlp                      -1              1              1       -1   \n",
       "Afinn                         -1              0              0        0   \n",
       "Vader                          0              0              0        0   \n",
       "SentiStrength                  1              1              1        1   \n",
       "Token Weight                  -1              2              2        0   \n",
       "Token Polarity     Weak Negative  Mild Positive  Mild Positive  Neutral   \n",
       "Sentence Polarity  Mild Positive           None           None     None   \n",
       "\n",
       "                                 4  \n",
       "Tokens                      stress  \n",
       "AllenNlp                        -1  \n",
       "Afinn                           -1  \n",
       "Vader                           -1  \n",
       "SentiStrength                   -1  \n",
       "Token Weight                    -4  \n",
       "Token Polarity     Strong Negative  \n",
       "Sentence Polarity             None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e88d9bad9746ccb1589bb7a4229d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Copy', style=ButtonStyle()), Dropdown(options=('Full DataFrame/Series', 'Ex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Tokens</td>\n",
       "      <td>stress</td>\n",
       "      <td>results</td>\n",
       "      <td>to</td>\n",
       "      <td>insomnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AllenNlp</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Afinn</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vader</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SentiStrength</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Weight</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Token Polarity</td>\n",
       "      <td>Strong Negative</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>Weak Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sentence Polarity</td>\n",
       "      <td>Mild Positive</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0              1              2  \\\n",
       "Tokens                      stress        results             to   \n",
       "AllenNlp                        -1              1              1   \n",
       "Afinn                           -1              0              0   \n",
       "Vader                           -1              0              0   \n",
       "SentiStrength                   -1              1              1   \n",
       "Token Weight                    -4              2              2   \n",
       "Token Polarity     Strong Negative  Mild Positive  Mild Positive   \n",
       "Sentence Polarity    Mild Positive           None           None   \n",
       "\n",
       "                               3  \n",
       "Tokens                  insomnia  \n",
       "AllenNlp                      -1  \n",
       "Afinn                         -1  \n",
       "Vader                          0  \n",
       "SentiStrength                  1  \n",
       "Token Weight                  -1  \n",
       "Token Polarity     Weak Negative  \n",
       "Sentence Polarity           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pd_replicator import replicator\n",
    "\n",
    "sum_list = []\n",
    "sum_tup = []\n",
    "limit= len(causalSentences)\n",
    "\n",
    "#print(limit)\n",
    "for i in range(0,limit):\n",
    "    #dataframe_WordLevel(i)\n",
    "    #print(len(allen_word[i]))\n",
    "    for j in range(0,len(allen_word[i])):\n",
    "        res = allen_word[i][j]+afinn_word[i][j]+vader_word[i][j]+senti_word[i][j]\n",
    "        #print(res)\n",
    "        sum_tup.append(res)\n",
    "    sum_list.append(sum_tup)\n",
    "    sum_tup = []\n",
    "    \n",
    "    \n",
    "tokenPolarity = []\n",
    "tokenPolarity_list = []\n",
    "\n",
    "for s in sum_list:\n",
    "    for weight in s:\n",
    "        if weight is -4:\n",
    "            tokenPolarity.append('Strong Negative') \n",
    "        elif weight is -3:\n",
    "            tokenPolarity.append('Moderate Negative')\n",
    "        elif weight is -2:\n",
    "            tokenPolarity.append('Mild Negative')\n",
    "        elif weight is -1:\n",
    "            tokenPolarity.append('Weak Negative')\n",
    "        elif weight is 0:\n",
    "            tokenPolarity.append('Neutral')\n",
    "        elif weight is 4:\n",
    "            tokenPolarity.append('Strong Positive')\n",
    "        elif weight is 3:\n",
    "            tokenPolarity.append('Moderate Positive')\n",
    "        elif weight is 2:\n",
    "            tokenPolarity.append('Mild Positive')\n",
    "        elif weight is 1:\n",
    "            tokenPolarity.append('Weak Positive')\n",
    "    tokenPolarity_list.append(tokenPolarity)\n",
    "    tokenPolarity = []\n",
    "\n",
    "maxPolarity = []\n",
    "maxPolarity_list = []\n",
    "\n",
    "for t in tokenPolarity_list:\n",
    "    counter=collections.Counter(t)\n",
    "    #print(counter)\n",
    "    #print(counter.most_common(1))\n",
    "    maxPolarity.append(counter.most_common(1)[0][0])\n",
    "    maxPolarity_list.append(maxPolarity)\n",
    "    maxPolarity = []\n",
    "\n",
    "#print(maxPolarity_list)\n",
    "    \n",
    "def dataframe_WordLevel(i):\n",
    "    df = pd.DataFrame([token_list[i],allen_word[i],afinn_word[i],vader_word[i],senti_word[i],sum_list[i],tokenPolarity_list[i],maxPolarity_list[i]], \n",
    "                      index =   ['Tokens','AllenNlp','Afinn','Vader','SentiStrength','Token Weight','Token Polarity','Sentence Polarity'])\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "    return df\n",
    "\n",
    "for i in range(0,limit):\n",
    "    df = dataframe_WordLevel(i)\n",
    "    #df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n",
    "    #display(df)\n",
    "    #df.to_excel(r'./Text/output.xlsx', index = False)\n",
    "    replicator(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Polarized/Modified Causal Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cg = nx.DiGraph()\n",
    "\n",
    "for ce in polarizedCauseffect:\n",
    "    s = str(ce[0])    \n",
    "    d = str(ce[2])\n",
    "    cg.add_node(s,id = str(s),title=str(s),x=615,y=200)\n",
    "    cg.add_node(d,id = str(d),title=str(d),x=615,y=200)\n",
    "    cg.add_edge(s, d, predicade=str(ce[1]))\n",
    "\n",
    "print(len(cg.nodes))\n",
    "print(len(cg.edges))\n",
    "\n",
    "pos = nx.spring_layout(cg, k=4, iterations=20)\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "nx.draw(cg, pos=pos, with_labels=True,  node_shape=\"s\",  node_color=\"none\", font_size=15,  font_color='royalblue', font_weight='bold',bbox=dict(facecolor=\"white\", alpha=0.4,boxstyle='round,pad=0.8'),labels={node: node for node in cg.nodes()},arrows=True, arrowsize=15,width=1)\n",
    "edge_labels = nx.get_edge_attributes(cg,'predicade')\n",
    "nx.draw_networkx_edge_labels(cmap, pos=pos, edge_labels = edge_labels, font_color='black',font_size=13)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the Causal Graph on \"Causal Graph.json\" file for semi-automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_graph.node_link_data(cg)\n",
    "\n",
    "\n",
    "out_file = open(\"Causal Graph.json\", \"w\")\n",
    "  \n",
    "json.dump(data, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2bbca7b25589987bc69e95430193d6372f81759d34e6ca1eac7aad3047d77e5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
